{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yaminipampana/FMML_LABS/blob/main/FMML_M1L2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Eu9VZbF01eq"
      },
      "source": [
        "# Machine Learning terms and metrics\n",
        "\n",
        "FMML Module 1, Lab 2\n",
        "\n",
        "In this lab, we will show a part of the ML pipeline by using the California Housing dataset. There are 20640 samples, each with 8 attributes like income of the block, age of the houses per district etc. The task is to predict the cost of the houses per district. We will use the scikit-learn library to load the data and perform some basic data preprocessing and model training. We will also show how to evaluate the model using some common metrics, split the data into training and testing sets, and use cross-validation to get a better estimate of the model's performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8qBvyEem0vLi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "rng = np.random.default_rng(seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LpqjN991GGJ",
        "outputId": "804fa71c-1805-4b3c-e7e7-3d99c7e31661"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".. _california_housing_dataset:\n",
            "\n",
            "California Housing dataset\n",
            "--------------------------\n",
            "\n",
            "**Data Set Characteristics:**\n",
            "\n",
            "    :Number of Instances: 20640\n",
            "\n",
            "    :Number of Attributes: 8 numeric, predictive attributes and the target\n",
            "\n",
            "    :Attribute Information:\n",
            "        - MedInc        median income in block group\n",
            "        - HouseAge      median house age in block group\n",
            "        - AveRooms      average number of rooms per household\n",
            "        - AveBedrms     average number of bedrooms per household\n",
            "        - Population    block group population\n",
            "        - AveOccup      average number of household members\n",
            "        - Latitude      block group latitude\n",
            "        - Longitude     block group longitude\n",
            "\n",
            "    :Missing Attribute Values: None\n",
            "\n",
            "This dataset was obtained from the StatLib repository.\n",
            "https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\n",
            "\n",
            "The target variable is the median house value for California districts,\n",
            "expressed in hundreds of thousands of dollars ($100,000).\n",
            "\n",
            "This dataset was derived from the 1990 U.S. census, using one row per census\n",
            "block group. A block group is the smallest geographical unit for which the U.S.\n",
            "Census Bureau publishes sample data (a block group typically has a population\n",
            "of 600 to 3,000 people).\n",
            "\n",
            "A household is a group of people residing within a home. Since the average\n",
            "number of rooms and bedrooms in this dataset are provided per household, these\n",
            "columns may take surprisingly large values for block groups with few households\n",
            "and many empty houses, such as vacation resorts.\n",
            "\n",
            "It can be downloaded/loaded using the\n",
            ":func:`sklearn.datasets.fetch_california_housing` function.\n",
            "\n",
            ".. topic:: References\n",
            "\n",
            "    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n",
            "      Statistics and Probability Letters, 33 (1997) 291-297\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dataset = datasets.fetch_california_housing()\n",
        "# Dataset description\n",
        "print(dataset.DESCR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCe1VNftevgE"
      },
      "source": [
        "Given below are the list of target values. These correspond to the house value derived considering all the 8 input features and are continuous values. We should use regression models to predict these values but we will start with a simple classification model for the sake of simplicity. We need to just round off the values to the nearest integer and use a classification model to predict the house value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8K0ggBOevgE",
        "outputId": "9a5cea00-39aa-4d19-fc23-46c708878069"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Orignal target values: [4.526 3.585 3.521 ... 0.923 0.847 0.894]\n",
            "Target values after conversion: [4 3 3 ... 0 0 0]\n",
            "Input variables shape: (20640, 8)\n",
            "Output variables shape: (20640,)\n"
          ]
        }
      ],
      "source": [
        "print(\"Orignal target values:\", dataset.target)\n",
        "\n",
        "dataset.target = dataset.target.astype(int)\n",
        "\n",
        "print(\"Target values after conversion:\", dataset.target)\n",
        "print(\"Input variables shape:\", dataset.data.shape)\n",
        "print(\"Output variables shape:\", dataset.target.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNx4174W5xRg"
      },
      "source": [
        "The simplest model to use for classification is the K-Nearest Neighbors model. We will use this model to predict the house value with a K value of 1. We will also use the accuracy metric to evaluate the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "07zpydQj1hIQ"
      },
      "outputs": [],
      "source": [
        "def NN1(traindata, trainlabel, query):\n",
        "    \"\"\"\n",
        "    This function takes in the training data, training labels and a query point\n",
        "    and returns the predicted label for the query point using the nearest neighbour algorithm\n",
        "\n",
        "    traindata: numpy array of shape (n,d) where n is the number of samples and d is the number of features\n",
        "    trainlabel: numpy array of shape (n,) where n is the number of samples\n",
        "    query: numpy array of shape (d,) where d is the number of features\n",
        "\n",
        "    returns: the predicted label for the query point which is the label of the training data which is closest to the query point\n",
        "    \"\"\"\n",
        "    diff = (\n",
        "        traindata - query\n",
        "    )  # find the difference between features. Numpy automatically takes care of the size here\n",
        "    sq = diff * diff  # square the differences\n",
        "    dist = sq.sum(1)  # add up the squares\n",
        "    label = trainlabel[np.argmin(dist)]\n",
        "    return label\n",
        "\n",
        "\n",
        "def NN(traindata, trainlabel, testdata):\n",
        "    \"\"\"\n",
        "    This function takes in the training data, training labels and test data\n",
        "    and returns the predicted labels for the test data using the nearest neighbour algorithm\n",
        "\n",
        "    traindata: numpy array of shape (n,d) where n is the number of samples and d is the number of features\n",
        "    trainlabel: numpy array of shape (n,) where n is the number of samples\n",
        "    testdata: numpy array of shape (m,d) where m is the number of test samples and d is the number of features\n",
        "\n",
        "    returns: the predicted labels for the test data which is the label of the training data which is closest to each test point\n",
        "    \"\"\"\n",
        "    predlabel = np.array([NN1(traindata, trainlabel, i) for i in testdata])\n",
        "    return predlabel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03JktkfIGaje"
      },
      "source": [
        "We will also define a 'random classifier', which randomly allots labels to each sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fogWAtjyGhAH"
      },
      "outputs": [],
      "source": [
        "def RandomClassifier(traindata, trainlabel, testdata):\n",
        "    \"\"\"\n",
        "    This function takes in the training data, training labels and test data\n",
        "    and returns the predicted labels for the test data using the random classifier algorithm\n",
        "\n",
        "    In reality, we don't need these arguments but we are passing them to keep the function signature consistent with other classifiers\n",
        "\n",
        "    traindata: numpy array of shape (n,d) where n is the number of samples and d is the number of features\n",
        "    trainlabel: numpy array of shape (n,) where n is the number of samples\n",
        "    testdata: numpy array of shape (m,d) where m is the number of test samples and d is the number of features\n",
        "\n",
        "    returns: the predicted labels for the test data which is a random label from the training data\n",
        "    \"\"\"\n",
        "\n",
        "    classes = np.unique(trainlabel)\n",
        "    rints = rng.integers(low=0, high=len(classes), size=len(testdata))\n",
        "    predlabel = classes[rints]\n",
        "    return predlabel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Hjf1KHs7fU5"
      },
      "source": [
        "We need a metric to evaluate the performance of the model. Let us define a metric 'Accuracy' to see how good our learning algorithm is. Accuracy is the ratio of the number of correctly classified samples to the total number of samples. The higher the accuracy, the better the algorithm. We will use the accuracy metric to evaluate and compate the performance of the K-Nearest Neighbors model and the random classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ouuCqWU07bz-"
      },
      "outputs": [],
      "source": [
        "def Accuracy(gtlabel, predlabel):\n",
        "    \"\"\"\n",
        "    This function takes in the ground-truth labels and predicted labels\n",
        "    and returns the accuracy of the classifier\n",
        "\n",
        "    gtlabel: numpy array of shape (n,) where n is the number of samples\n",
        "    predlabel: numpy array of shape (n,) where n is the number of samples\n",
        "\n",
        "    returns: the accuracy of the classifier which is the number of correct predictions divided by the total number of predictions\n",
        "    \"\"\"\n",
        "    assert len(gtlabel) == len(\n",
        "        predlabel\n",
        "    ), \"Length of the ground-truth labels and predicted labels should be the same\"\n",
        "    correct = (\n",
        "        gtlabel == predlabel\n",
        "    ).sum()  # count the number of times the groundtruth label is equal to the predicted label.\n",
        "    return correct / len(gtlabel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vJFwBFa9Klw"
      },
      "source": [
        "Let us make a function to split the dataset with the desired probability. We will use this function to split the dataset into training and testing sets. We will use the training set to train the model and the testing set to evaluate the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Ko0VzpSM2Tdi"
      },
      "outputs": [],
      "source": [
        "def split(data, label, percent):\n",
        "    # generate a random number for each sample\n",
        "    rnd = rng.random(len(label))\n",
        "    split1 = rnd < percent\n",
        "    split2 = rnd >= percent\n",
        "\n",
        "    split1data = data[split1, :]\n",
        "    split1label = label[split1]\n",
        "    split2data = data[split2, :]\n",
        "    split2label = label[split2]\n",
        "    return split1data, split1label, split2data, split2label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcK3LEAJ_LGC"
      },
      "source": [
        "We will reserve 20% of our dataset as the test set. We will not change this portion throughout our experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBZkHBLJ1iU-",
        "outputId": "28eac3c5-ff8d-41c1-a361-12d9ce0e1ada"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of test samples: 4144\n",
            "Number of train samples: 16496\n",
            "Percent of test data: 20.07751937984496 %\n"
          ]
        }
      ],
      "source": [
        "testdata, testlabel, alltraindata, alltrainlabel = split(\n",
        "    dataset.data, dataset.target, 20 / 100\n",
        ")\n",
        "print(\"Number of test samples:\", len(testlabel))\n",
        "print(\"Number of train samples:\", len(alltrainlabel))\n",
        "print(\"Percent of test data:\", len(testlabel) * 100 / len(dataset.target), \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6Ss0Z6IAGNV"
      },
      "source": [
        "## Experiments with splits\n",
        "\n",
        "Let us reserve some of our train data as a validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "WFew2iry_7W7"
      },
      "outputs": [],
      "source": [
        "traindata, trainlabel, valdata, vallabel = split(\n",
        "    alltraindata, alltrainlabel, 75 / 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60hiu4clFN1i"
      },
      "source": [
        "What is the accuracy of our classifiers on the train dataset?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBlZDTHUFTZx",
        "outputId": "4004d745-1342-498a-c334-c457e42978d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training accuracy using nearest neighbour algorithm: 100.0 %\n",
            "Training accuracy using random classifier:  16.4375808538163 %\n"
          ]
        }
      ],
      "source": [
        "trainpred = NN(traindata, trainlabel, traindata)\n",
        "trainAccuracy = Accuracy(trainlabel, trainpred)\n",
        "print(\"Training accuracy using nearest neighbour algorithm:\", trainAccuracy*100, \"%\")\n",
        "\n",
        "trainpred = RandomClassifier(traindata, trainlabel, traindata)\n",
        "trainAccuracy = Accuracy(trainlabel, trainpred)\n",
        "print(\"Training accuracy using random classifier: \", trainAccuracy*100, \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7h08-9gJDtSy"
      },
      "source": [
        "For nearest neighbour, the train accuracy is always 1. The accuracy of the random classifier is close to 1/(number of classes) which is 0.1666 in our case. This is because the random classifier randomly assigns a label to each sample and the probability of assigning the correct label is 1/(number of classes). Let us predict the labels for our validation set and get the accuracy. This accuracy is a good estimate of the accuracy of our model on unseen data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4h7bXoW_2H3v",
        "outputId": "273b34d2-7249-43e2-e13d-0b5ebc74ab53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy using nearest neighbour algorithm: 34.10852713178294 %\n",
            "Validation accuracy using random classifier: 16.884689922480618 %\n"
          ]
        }
      ],
      "source": [
        "valpred = NN(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy using nearest neighbour algorithm:\", valAccuracy*100, \"%\")\n",
        "\n",
        "\n",
        "valpred = RandomClassifier(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy using random classifier:\", valAccuracy*100, \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "py9bLguFEjfg"
      },
      "source": [
        "Validation accuracy of nearest neighbour is considerably less than its train accuracy while the validation accuracy of random classifier is the same. However, the validation accuracy of nearest neighbour is twice that of the random classifier. Now let us try another random split and check the validation accuracy. We will see that the validation accuracy changes with the split. This is because the validation set is small and the accuracy is highly dependent on the samples in the validation set. We can get a better estimate of the accuracy by using cross-validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujm3cyYzEntE",
        "outputId": "23d349a8-ee3c-4dc8-eec4-5b655867bfd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy using nearest neighbour algorithm: 34.048257372654156 %\n"
          ]
        }
      ],
      "source": [
        "traindata, trainlabel, valdata, vallabel = split(\n",
        "    alltraindata, alltrainlabel, 75 / 100)\n",
        "valpred = NN(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy using nearest neighbour algorithm:\", valAccuracy*100, \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSOx7U83EKie"
      },
      "source": [
        "You can run the above cell multiple times to try with different random splits.\n",
        "We notice that the accuracy is different for each run, but close together.\n",
        "\n",
        "Now let us compare it with the accuracy we get on the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNEZ5ToYBEDW",
        "outputId": "5b1ea4d8-1a24-4c0f-8f6f-f17dba0c2b92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 34.91795366795367 %\n"
          ]
        }
      ],
      "source": [
        "testpred = NN(alltraindata, alltrainlabel, testdata)\n",
        "testAccuracy = Accuracy(testlabel, testpred)\n",
        "\n",
        "print(\"Test accuracy:\", testAccuracy*100, \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3dGD531K3gH"
      },
      "source": [
        "### Try it out for yourself and answer:\n",
        "1. How is the accuracy of the validation set affected if we increase the percentage of validation set? What happens when we reduce it?\n",
        "2. How does the size of the train and validation set affect how well we can predict the accuracy on the test set using the validation set?\n",
        "3. How does the size of the train and validation set affect how well we can predict the accuracy on the test set using the validation set?\n",
        "\n",
        "Answer for both nearest neighbour and random classifier. You can note down the values for your experiments and plot a graph using  <a href=https://matplotlib.org/stable/gallery/lines_bars_and_markers/step_demo.html#sphx-glr-gallery-lines-bars-and-markers-step-demo-py>plt.plot<href>. Check also for extreme values for splits, like 99.9% or 0.1%"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. Increasing the Percentage of the Validation Set\n",
        "\n",
        " Smaller Training Set: Increasing the size of the validation set reduces the amount of data available for training. Since machine learning models learn patterns from training data, having less data may result in the model learning less effectively, potentially reducing its ability to generalize well.\n",
        "\n",
        " Validation Set Accuracy:\n",
        "\n",
        " Stabilization of Metrics: With a larger validation set, performance metrics (like accuracy, precision, recall, etc.) are typically more stable. Larger datasets better represent the overall population, leading to a more reliable estimation of the model's true performance.\n",
        "\n",
        " Possibly Lower Model Performance: If the model was trained on a reduced training set, the performance metrics might slightly decrease because the model had fewer samples to learn from.\n",
        "2. Reducing the Percentage of the Validation Set\n",
        "\n",
        " Larger Training Set: With more training data, the model generally learns more patterns, potentially improving its performance on unseen data. A larger training set helps prevent underfitting.\n",
        "\n",
        " Validation Set Accuracy:\n",
        "\n",
        " Higher Variance in Metrics: A smaller validation set may not represent the overall distribution of the data well, leading to high variability in performance metrics. This makes it harder to get an accurate estimate of how the model will perform on new, unseen data.\n",
        "\n",
        " Overfitting Risks: If the validation set is too small, the model may overfit the training set, and performance metrics on the validation set may not reflect the model’s true generalization ability.\n",
        "\n",
        " Experimentation with Different Splits\n",
        "\n",
        " Common training-validation splits include 80-20, 90-10, and 70-30. For example:\n",
        "\n",
        " 80-20 Split: A well-balanced trade-off between training and validation. There's enough data for the model to learn, and the validation metrics are relatively reliable.\n",
        "\n",
        " 90-10 Split: A larger training set benefits the learning process, but validation metrics may show higher variability due to a smaller validation set.\n",
        "\n",
        " 70-30 Split: A larger validation set gives more confidence in validation metrics but may reduce model learning if the training set becomes too small.\n",
        "\n",
        " Metrics Impacted\n",
        "\n",
        " Validation Accuracy: With more data in the validation set, accuracy becomes more reliable, but the model's ability to generalize may suffer if there’s not enough training data.\n",
        " Precision/Recall: These metrics might fluctuate more with smaller validation sets due to imbalance or underrepresentation of certain classes.\n",
        " F1-Score: A balance between precision and recall, it can also be sensitive to validation set size, especially in imbalanced datasets.\n"
      ],
      "metadata": {
        "id": "MOyRVsvEK_VB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The size of the training and validation sets directly impacts how well we can predict the accuracy on the test set using the validation set, because these sets control both model learning and evaluation. Here’s a breakdown of how they affect the prediction of test set accuracy:\n",
        "\n",
        "\n",
        "1. Impact of Training Set Size on Predicting Test Set Accuracy\n",
        "\n",
        " Larger Training Set:\n",
        "\n",
        " Better Learning: A larger training set allows the model to learn more patterns and generalize better. This leads to better performance on unseen data, including both the validation and test sets.\n",
        "\n",
        " More Reliable Validation Accuracy: If the model learns well from a large training set, its performance on the validation set will more closely mirror its performance on the test set.\n",
        "\n",
        " More Robust Models: A larger training set reduces overfitting, as the model is less likely to memorize the training data and instead captures the underlying patterns, making validation accuracy a good proxy for test accuracy.\n",
        "\n",
        " Smaller Training Set:\n",
        "\n",
        " Underfitting Risk: With less data, the model may underfit, meaning it doesn’t capture enough of the patterns in the data. As a result, the validation accuracy may be lower, and there will be less confidence that it reflects the test set accuracy.\n",
        "\n",
        " Greater Performance Gap: A smaller training set might cause a gap between validation accuracy and test accuracy. If the model is not well-trained, it may perform poorly on the test set even if it seems to perform decently on a small validation set.\n",
        "\n",
        "2. Impact of Validation Set Size on Predicting Test Set Accuracy\n",
        "\n",
        " Larger Validation Set:\n",
        "\n",
        " More Reliable Estimate: A larger validation set offers a better estimate of the model’s performance, as it represents a broader portion of the data. If the validation set accurately reflects the test set distribution, its accuracy will be closer to the test set accuracy.\n",
        "\n",
        " Less Variability: With more data in the validation set, the variance in accuracy metrics (e.g., accuracy, precision, recall) is lower. This reduces the likelihood of overestimating or underestimating the model's performance.\n",
        "\n",
        " Improved Generalization: A larger validation set minimizes the likelihood of spurious results due to a small sample size, leading to more confidence that the validation accuracy will closely mirror the test accuracy.\n",
        "\n",
        " Smaller Validation Set:\n",
        "\n",
        " Higher Variability: A smaller validation set has more variance in performance metrics due to fewer data points. This increases the risk that validation accuracy is either overestimated or underestimated, leading to poor predictions about test set accuracy.\n",
        "\n",
        " Risk of Overfitting to Validation Data: If the validation set is too small, the model might perform well on the validation set (due to being accidentally well-suited to the small sample), but poorly on the test set, leading to inaccurate predictions of performance on unseen data.\n",
        "\n",
        "3. Training-Validation-Testing Relationships\n",
        "\n",
        " Large Training Set + Small Validation Set: This setup might produce a well-trained model, but the small validation set could give misleading performance metrics (high variance), making it hard to confidently predict the model’s accuracy on the test set.\n",
        "\n",
        " Small Training Set + Large Validation Set: This might lead to a model that underfits the data, giving lower validation accuracy, which might underestimate test accuracy. A well-represented validation set may reflect test accuracy better, but if the model hasn't learned well due to the small training set, both validation and test accuracy may be suboptimal.\n",
        "\n",
        " Balanced Training and Validation Set: With a balanced split (e.g., 80-20), there’s enough data for the model to learn well and for the validation set to give an accurate estimate of the model's performance on unseen test data.\n",
        "\n",
        " Practical Implications\n",
        "\n",
        "  Smaller Datasets: In smaller datasets, finding a good balance between training and validation data is crucial. In these cases, using techniques like cross-validation (where multiple subsets are used as validation sets) is helpful in accurately predicting test set accuracy.\n",
        "\n",
        " Larger Datasets: In larger datasets, a smaller validation set (e.g., 10%) may still yield reliable estimates of test set accuracy since both training and validation data represent enough of the data distribution.\n"
      ],
      "metadata": {
        "id": "cG20W1vjMFk1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The size of the training and validation sets directly impacts how well we can predict the accuracy on the test set using the validation set, because these sets control both model learning and evaluation. Here’s a breakdown of how they affect the prediction of test set accuracy:\n",
        "\n",
        "\n",
        "1. Impact of Training Set Size on Predicting Test Set Accuracy\n",
        "\n",
        " Larger Training Set:\n",
        "\n",
        " Better Learning: A larger training set allows the model to learn more patterns and generalize better. This leads to better performance on unseen data, including both the validation and test sets.\n",
        "\n",
        " More Reliable Validation Accuracy: If the model learns well from a large training set, its performance on the validation set will more closely mirror its performance on the test set.\n",
        "\n",
        " More Robust Models: A larger training set reduces overfitting, as the model is less likely to memorize the training data and instead captures the underlying patterns, making validation accuracy a good proxy for test accuracy.\n",
        "\n",
        " Smaller Training Set:\n",
        "\n",
        " Underfitting Risk: With less data, the model may underfit, meaning it doesn’t capture enough of the patterns in the data. As a result, the validation accuracy may be lower, and there will be less confidence that it reflects the test set accuracy.\n",
        "\n",
        " Greater Performance Gap: A smaller training set might cause a gap between validation accuracy and test accuracy. If the model is not well-trained, it may perform poorly on the test set even if it seems to perform decently on a small validation set.\n",
        "\n",
        "2. Impact of Validation Set Size on Predicting Test Set Accuracy\n",
        "\n",
        " Larger Validation Set:\n",
        "\n",
        " More Reliable Estimate: A larger validation set offers a better estimate of the model’s performance, as it represents a broader portion of the data. If the validation set accurately reflects the test set distribution, its accuracy will be closer to the test set accuracy.\n",
        "\n",
        " Less Variability: With more data in the validation set, the variance in accuracy metrics (e.g., accuracy, precision, recall) is lower. This reduces the likelihood of overestimating or underestimating the model's performance.\n",
        "\n",
        " Improved Generalization: A larger validation set minimizes the likelihood of spurious results due to a small sample size, leading to more confidence that the validation accuracy will closely mirror the test accuracy.\n",
        "\n",
        " Smaller Validation Set:\n",
        "\n",
        " Higher Variability: A smaller validation set has more variance in performance metrics due to fewer data points. This increases the risk that validation accuracy is either overestimated or underestimated, leading to poor predictions about test set accuracy.\n",
        "\n",
        " Risk of Overfitting to Validation Data: If the validation set is too small, the model might perform well on the validation set (due to being accidentally well-suited to the small sample), but poorly on the test set, leading to inaccurate predictions of performance on unseen data.\n",
        "\n",
        "3. Training-Validation-Testing Relationships\n",
        "\n",
        " Large Training Set + Small Validation Set: This setup might produce a well-trained model, but the small validation set could give misleading performance metrics (high variance), making it hard to confidently predict the model’s accuracy on the test set.\n",
        "\n",
        " Small Training Set + Large Validation Set: This might lead to a model that underfits the data, giving lower validation accuracy, which might underestimate test accuracy. A well-represented validation set may reflect test accuracy better, but if the model hasn't learned well due to the small training set, both validation and test accuracy may be suboptimal.\n",
        "\n",
        " Balanced Training and Validation Set: With a balanced split (e.g., 80-20), there’s enough data for the model to learn well and for the validation set to give an accurate estimate of the model's performance on unseen test data.\n",
        "\n",
        " Practical Implications\n",
        "\n",
        "  Smaller Datasets: In smaller datasets, finding a good balance between training and validation data is crucial. In these cases, using techniques like cross-validation (where multiple subsets are used as validation sets) is helpful in accurately predicting test set accuracy.\n",
        "\n",
        " Larger Datasets: In larger datasets, a smaller validation set (e.g., 10%) may still yield reliable estimates of test set accuracy since both training and validation data represent enough of the data distribution.\n"
      ],
      "metadata": {
        "id": "zBbW6cP-N2zn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9zvdYY6evgI"
      },
      "source": [
        "> Exercise: Try to implement a 3 nearest neighbour classifier and compare the accuracy of the 1 nearest neighbour classifier and the 3 nearest neighbour classifier on the test dataset. You can use the KNeighborsClassifier class from the scikit-learn library to implement the K-Nearest Neighbors model. You can set the number of neighbors using the n_neighbors parameter. You can also use the accuracy_score function from the scikit-learn library to calculate the accuracy of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnYvkAZLQY7h"
      },
      "source": [
        "## Multiple Splits\n",
        "\n",
        "One way to get more accurate estimates for the test accuracy is by using <b>cross-validation</b>. Here, we will try a simple version, where we do multiple train/val splits and take the average of validation accuracies as the test accuracy estimation. Here is a function for doing this. Note that this function will take a long time to execute. You can reduce the number of splits to make it faster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "E4nGCUQXBTzo"
      },
      "outputs": [],
      "source": [
        "def AverageAccuracy(alldata, alllabel, splitpercent, iterations, classifier=NN):\n",
        "    \"\"\"\n",
        "    This function takes in the data, labels, split percentage, number of iterations and classifier function\n",
        "    and returns the average accuracy of the classifier\n",
        "\n",
        "    alldata: numpy array of shape (n,d) where n is the number of samples and d is the number of features\n",
        "    alllabel: numpy array of shape (n,) where n is the number of samples\n",
        "    splitpercent: float which is the percentage of data to be used for training\n",
        "    iterations: int which is the number of iterations to run the classifier\n",
        "    classifier: function which is the classifier function to be used\n",
        "\n",
        "    returns: the average accuracy of the classifier\n",
        "    \"\"\"\n",
        "    accuracy = 0\n",
        "    for ii in range(iterations):\n",
        "        traindata, trainlabel, valdata, vallabel = split(\n",
        "            alldata, alllabel, splitpercent\n",
        "        )\n",
        "        valpred = classifier(traindata, trainlabel, valdata)\n",
        "        accuracy += Accuracy(vallabel, valpred)\n",
        "    return accuracy / iterations  # average of all accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3qtNar7Bbik",
        "outputId": "3d5fdc18-3a7d-49b1-cbd8-97f3fae82858"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average validation accuracy: 33.58463539517022 %\n",
            "Test accuracy: 34.91795366795367 %\n"
          ]
        }
      ],
      "source": [
        "avg_acc = AverageAccuracy(alltraindata, alltrainlabel, 75 / 100, 10, classifier=NN)\n",
        "print(\"Average validation accuracy:\", avg_acc*100, \"%\")\n",
        "testpred = NN(alltraindata, alltrainlabel, testdata)\n",
        "\n",
        "print(\"Test accuracy:\", Accuracy(testlabel, testpred)*100, \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33GIn4x5VH-d"
      },
      "source": [
        "This is a very simple way of doing cross-validation. There are many well-known algorithms for cross-validation, like k-fold cross-validation, leave-one-out etc. This will be covered in detail in a later module. For more information about cross-validation, check <a href=https://en.wikipedia.org/wiki/Cross-validation_(statistics)>Cross-validatioin (Wikipedia)</a>\n",
        "\n",
        "### Questions\n",
        "1. Does averaging the validation accuracy across multiple splits give more consistent results?\n",
        "2. Does it give more accurate estimate of test accuracy?\n",
        "3. What is the effect of the number of iterations on the estimate? Do we get a better estimate with higher iterations?\n",
        "4. Consider the results you got for the previous questions. Can we deal with a very small train dataset or validation dataset by increasing the iterations?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Averaging Across Multiple Splits Provides Consistency\n",
        "\n",
        " Reduces Variance: When you evaluate a model on a single validation split, the performance may vary depending on how representative that specific subset is of the overall data. Some splits might be easier for the model to predict than others, leading to variability in performance metrics. By averaging the validation accuracy across multiple splits, you reduce this variance and get a more reliable estimate of the model's performance.\n",
        "\n",
        " More Representative of Data Distribution: Each split (or fold) provides a different subset of data for validation. By training and evaluating the model on different subsets, you ensure that all parts of the dataset contribute to both training and validation. This makes the average validation accuracy a better reflection of the model’s ability to generalize across the entire dataset.\n",
        "\n",
        " Mitigates Bias from a Single Split: A single validation split might contain outliers or an unrepresentative sample of the data, which can bias the accuracy estimate. Averaging across multiple splits helps cancel out the effect of such anomalies, giving a more stable and consistent result.\n",
        "2. k-Fold Cross-Validation for Multiple Splits\n",
        "\n",
        " k-Fold Cross-Validation is a commonly used method to average validation accuracy across multiple splits. In this technique:\n",
        "\n",
        " The data is divided into k equal-sized \"folds\".\n",
        "\n",
        " The model is trained on k−1 folds and validated on the remaining fold.\n",
        "\n",
        " This process is repeated k times, each time using a different fold as the validation set.\n",
        "\n",
        " The final validation accuracy is the average accuracy across all folds, which provides a more reliable performance estimate.\n",
        "\n",
        " The value of k (e.g., 5, 10) controls how many splits are performed. Higher values of k provide more splits but require more computation.\n",
        "3. Leave-One-Out Cross-Validation (LOO-CV)\n",
        "\n",
        " A special case of k-fold cross-validation is leave-one-out cross-validation (LOO-CV), where k equals the number of data points. This method provides the most exhaustive validation, as the model is trained on all but one data point and validated on the remaining one, repeated for every point in the dataset.\n",
        "\n",
        "  While this reduces variance in performance estimates, it can be computationally expensive for large datasets.\n",
        "4. Effects on Model Selection and Hyperparameter Tuning\n",
        "\n",
        " More Reliable Hyperparameter Tuning: When hyperparameters are tuned based on the performance of a single validation set, there's a risk that the chosen hyperparameters may be overfitted to that specific split. Averaging across multiple splits mitigates this risk, leading to hyperparameter choices that generalize better across different data.\n",
        "\n",
        "  More Stable Model Selection: Similarly, choosing between different models based on the average performance over multiple splits ensures that the selected model performs consistently well across different subsets of the data, rather than excelling on just one.\n",
        "5. Considerations for Consistent Results\n",
        "\n",
        " Number of Splits (k): The value of k in k-fold cross-validation affects the consistency of results. A larger value of k (e.g., 10) typically provides a more accurate estimate of the model's performance but requires more computation. A lower value (e.g., 5) reduces the computational cost but may increase variability.\n",
        "\n",
        " Dataset Size: For smaller datasets, cross-validation is particularly important because a single split may not represent the overall data well. Larger datasets generally allow more reliable performance estimates even with a single split.\n"
      ],
      "metadata": {
        "id": "aek1Fd8kPtk0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. *Reduced Variance*: By splitting the data multiple times, the model is trained and tested on different subsets of the data. This reduces the variance of the performance estimate, leading to more robust and reliable results.\n",
        "\n",
        "2. *Use of Full Dataset*: With multiple splits, every observation in the dataset gets a chance to be in both the training and testing sets, providing a more comprehensive view of model performance.\n",
        "\n",
        "3. *Less Bias*: A single train-test split can lead to a biased estimate of performance if the data is not representative. Multiple splits help mitigate this issue.\n",
        "\n",
        "For example, *k-fold cross-validation* divides the data into k subsets, trains the model on k-1 folds, and tests on the remaining fold. This process repeats k times, and the final accuracy is averaged across all runs, giving a more accurate estimate."
      ],
      "metadata": {
        "id": "S1Oy72yqSWLy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. *More Accurate Estimate*: Higher iterations (e.g., more folds in k-fold cross-validation) usually provide a more accurate estimate of model performance. This is because each fold has more data for both training and testing, leading to better utilization of the dataset and a more stable estimate of the model's performance.\n",
        "\n",
        "2. *Trade-Off with Computation: While more iterations lead to better estimates, they also increase the computational cost. For example, in k-fold cross-validation, the number of iterations is equal to *k, and each iteration involves training and testing the model. Thus, more folds mean more computations.\n",
        "\n",
        "3. *Diminishing Returns*: After a certain point, increasing the number of folds yields diminishing returns in terms of improving the estimate. For instance, in 10-fold cross-validation, increasing the number of folds beyond this may not provide substantial benefits, but will increase computational demands."
      ],
      "metadata": {
        "id": "QaMVfbmXTvER"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Increasing the number of iterations in cross-validation (like using more folds) helps in better utilizing the limited data but doesn’t fully solve the problem of having very small datasets. Here’s a closer look:\n",
        "\n",
        "1. *Better Data Utilization*: With more folds (higher iterations), each data point gets to be in both the training and validation sets across different iterations. This means every sample contributes to the training and validation processes multiple times, potentially leading to a more reliable estimate of model performance.\n",
        "\n",
        "2. *Stability in Estimates*: More iterations (or folds) can give you a more stable estimate of model performance by reducing variance due to the specific choice of training and validation splits. This is particularly useful when the dataset is small.\n",
        "\n",
        "3. *Not a Complete Solution*:\n",
        "   - *Overfitting Risks*: Even with increased iterations, a very small dataset may still lead to overfitting, as the model may learn noise or specific patterns that do not generalize well.\n",
        "   - *Computational Cost*: Increasing the number of folds increases the computational load, which might be inefficient if the dataset is already very small.\n",
        "\n",
        "4. *Other Strategies*: For very small datasets, consider additional strategies like:\n",
        "   - *Data Augmentation*: Generating more data from existing samples through techniques such as transformations or synthetic data.\n",
        "   - *Regularization*: Applying techniques that prevent overfitting by penalizing overly complex models.\n",
        "   - *Transfer Learning*: Leveraging pre-trained models and fine-tuning them on your small dataset."
      ],
      "metadata": {
        "id": "ZKu_1R8-UfhH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-SBxy1qevgJ"
      },
      "source": [
        "> Exercise: How does the accuracy of the 3 nearest neighbour classifier change with the number of splits? How is it affected by the split size? Compare the results with the 1 nearest neighbour classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To analyze how the accuracy of the k-nearest neighbors (k-NN) classifier changes with the number of splits and split size, and to compare the results between 1-nearest neighbor (1-NN) and 3-nearest neighbor (3-NN), follow these steps:\n",
        "\n",
        "### 1. *Dataset Preparation:*\n",
        "   - Use a dataset appropriate for classification, preferably one with a moderate size to observe the effects more clearly.\n",
        "\n",
        "### 2. *Perform Cross-Validation:*\n",
        "   - *1-NN Classifier:*\n",
        "     - Perform cross-validation with different numbers of splits (e.g., 5-fold, 10-fold) and compute the accuracy for each.\n",
        "   - *3-NN Classifier:*\n",
        "     - Repeat the above steps for the 3-NN classifier.\n",
        "\n",
        "### 3. *Analysis:*\n",
        "\n",
        "#### *Accuracy with Number of Splits:*\n",
        "   - *Effect of More Splits:*\n",
        "     - Generally, increasing the number of splits (e.g., moving from 5-fold to 10-fold cross-validation) can provide a more reliable estimate of accuracy as it reduces variance and ensures that more data is used for both training and validation.\n",
        "   - *1-NN vs. 3-NN:*\n",
        "     - *1-NN* tends to have high variance and can be sensitive to noise in the training data. Its performance may fluctuate more with different splits because it relies on a single nearest neighbor.\n",
        "     - *3-NN* typically smooths out the decision boundary by considering the average class of the 3 nearest neighbors, potentially leading to more stable performance across different splits.\n",
        "\n",
        "#### *Accuracy with Split Size:*\n",
        "   - *Effect of Split Size:*\n",
        "     - Smaller splits (larger number of folds) mean that each validation set is smaller, which might increase variability in the accuracy estimates. However, the model benefits from having more training data.\n",
        "     - Larger splits (fewer folds) result in larger validation sets and smaller training sets, which may make the accuracy estimate more stable but could suffer from reduced training data.\n",
        "\n",
        "   - *1-NN vs. 3-NN:*\n",
        "     - *1-NN* may be more sensitive to the size of the validation set due to its dependence on the closest single point, leading to higher variability in accuracy.\n",
        "     - *3-NN* tends to be less sensitive to individual data points, so its accuracy might be more stable with different split sizes.\n",
        "\n",
        "### 4. *Comparison:*\n",
        "\n",
        "- *1-NN*:\n",
        "  - Often has higher variance in accuracy with different splits due to overfitting to the nearest neighbor. It can perform very well on training data but may not generalize well, especially with small validation sets.\n",
        "\n",
        "- *3-NN*:\n",
        "  - Provides more stable performance across different splits as it averages out the influence of nearest neighbors. It may have better generalization and be less sensitive to the exact split of the data compared to 1-NN."
      ],
      "metadata": {
        "id": "SKCL2hK3V3sp"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}